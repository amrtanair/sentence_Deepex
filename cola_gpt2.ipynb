{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amrtanair/sentence_Deepex/blob/master/cola_gpt2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "CFr2NQT_jPP8"
      },
      "outputs": [],
      "source": [
        "# code from stack overflow that allows code output to overflow to next line\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "def set_css():\n",
        "  display(HTML('''\n",
        "  <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "  </style>\n",
        "  '''))\n",
        "get_ipython().events.register('pre_run_cell', set_css)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 481
        },
        "id": "02HUgBtg26Od",
        "outputId": "966560f8-1c5a-48f6-d4c7-a8a1500eeb12"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wget in /usr/local/lib/python3.10/dist-packages (3.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Downloading dataset...\n",
            "GPU: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "!pip install wget\n",
        "!pip install transformers\n",
        "\n",
        "import wget\n",
        "import os\n",
        "\n",
        "import time\n",
        "import datetime\n",
        "import random\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from transformers import (set_seed,\n",
        "                          TrainingArguments,\n",
        "                          Trainer,\n",
        "                          GPT2Config,\n",
        "                          GPT2Tokenizer,\n",
        "                          AdamW,\n",
        "                          get_linear_schedule_with_warmup,\n",
        "                          GPT2ForSequenceClassification)\n",
        "\n",
        "print('Downloading dataset...')\n",
        "\n",
        "url = 'https://nyu-mll.github.io/CoLA/cola_public_1.1.zip'\n",
        "\n",
        "if not os.path.exists('./cola_public_1.1.zip'):\n",
        "    wget.download(url, './cola_public_1.1.zip')\n",
        "\n",
        "if not os.path.exists('./cola_public/'):\n",
        "    !unzip cola_public_1.1.zip\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print('GPU:', torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    print('Using CPU')\n",
        "    device = torch.device(\"cpu\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "1AAsXZ_-6ukb",
        "outputId": "b6edf28a-78b3-4bd0-a009-1841e902b63f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#hyperparameters\n",
        "\n",
        "epochs = 6\n",
        "batch_size = 24\n",
        "max_length = 48\n",
        "model_name_or_path = 'gpt2-medium'\n",
        "warmup = True\n",
        "learning_rate = 2e-05\n",
        "seed_val = 42\n",
        "\n",
        "labels_ids = {\n",
        "    \"0\" : 0,\n",
        "    \"1\" : 1,\n",
        "    }\n",
        "\n",
        "n_labels = len(labels_ids)\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "\ttorch.cuda.manual_seed_all(seed_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "wBkZn18T7gxF",
        "outputId": "24eb9ece-ce82-4247-ea03-0ee794f3364f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "class PromptDataset(Dataset):\n",
        "  def __init__(self, path, use_tokenizer, test = False):\n",
        "    df = pd.read_csv(path, delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
        "    df['label'] = df['label'].astype(str)\n",
        "    grouped = df.groupby('label')\n",
        "    dfs = []\n",
        "    min_group_size = grouped.size().min()\n",
        "\n",
        "    for _, group_df in grouped:\n",
        "        dfs.append(group_df.iloc[:min_group_size])\n",
        "\n",
        "    df = pd.concat(dfs, ignore_index=True)\n",
        "    df = df.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "    self.dataframe = df\n",
        "    # if not test:\n",
        "    #     self.dataframe = self.dataframe.head(int(0.85 * len(self.dataframe)))\n",
        "    self.texts = self.dataframe.sentence.values\n",
        "    self.dataframe['label'] = self.dataframe['label'].astype(str)\n",
        "    self.labels = self.dataframe.label.values\n",
        "\n",
        "    self.n_examples = len(self.labels)\n",
        "    return\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.n_examples\n",
        "\n",
        "  def __getitem__(self, item):\n",
        "    return {'text':self.texts[item],\n",
        "            'label':self.labels[item]}\n",
        "\n",
        "class PromptCollator(object):\n",
        "    def __init__(self, use_tokenizer, labels_encoder, max_sequence_len=None):\n",
        "        self.use_tokenizer = use_tokenizer\n",
        "        self.max_sequence_len = use_tokenizer.model_max_length if max_sequence_len is None else max_sequence_len\n",
        "        self.labels_encoder = labels_encoder\n",
        "\n",
        "        return\n",
        "\n",
        "    def __call__(self, sequences):\n",
        "        texts = [sequence['text'] for sequence in sequences]\n",
        "        labels = [sequence['label'] for sequence in sequences]\n",
        "        labels = [self.labels_encoder[label] for label in labels]\n",
        "\n",
        "        inputs = self.use_tokenizer(text=texts, return_tensors=\"pt\", padding=True, truncation=True,  max_length=self.max_sequence_len)\n",
        "        inputs.update({'labels': torch.tensor(labels)})\n",
        "\n",
        "        return inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "XPv1FOF28MT6",
        "outputId": "9e516b83-ce74-4a34-a25a-05f1bc9c8094"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "def train(dataloader, optimizer_, device_, scheduler_):\n",
        "    global model\n",
        "    predictions_labels = []\n",
        "    true_labels = []\n",
        "\n",
        "    total_loss = 0\n",
        "    model.train()\n",
        "\n",
        "    for batch in dataloader:\n",
        "        true_labels += batch['labels'].numpy().flatten().tolist()\n",
        "        batch = {k:v.type(torch.long).to(device_) for k,v in batch.items()}\n",
        "        model.zero_grad()\n",
        "        outputs = model(**batch)\n",
        "        loss, logits = outputs[:2]\n",
        "        total_loss += loss.item()\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer_.step()\n",
        "        scheduler_.step()\n",
        "\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        predictions_labels += logits.argmax(axis=-1).flatten().tolist()\n",
        "    avg_epoch_loss = total_loss / len(dataloader)\n",
        "    return true_labels, predictions_labels, avg_epoch_loss\n",
        "\n",
        "def validation(dataloader, device_):\n",
        "    global model\n",
        "    predictions_labels = []\n",
        "    true_labels = []\n",
        "    total_loss = 0\n",
        "    model.eval()\n",
        "\n",
        "    for batch in dataloader:\n",
        "        true_labels += batch['labels'].numpy().flatten().tolist()\n",
        "        batch = {k:v.type(torch.long).to(device_) for k,v in batch.items()}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**batch)\n",
        "            loss, logits = outputs[:2]\n",
        "            logits = logits.detach().cpu().numpy()\n",
        "            total_loss += loss.item()\n",
        "            predict_content = logits.argmax(axis=-1).flatten().tolist()\n",
        "            predictions_labels += predict_content\n",
        "    avg_epoch_loss = total_loss / len(dataloader)\n",
        "    return true_labels, predictions_labels, avg_epoch_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 910
        },
        "id": "WFc4dN_A_aRr",
        "outputId": "0473b86f-532d-424b-851e-a1e996a604bb"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model and tokenizer\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2-medium and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created `train_dataset` with 5056 examples!\n",
            "Created `train_dataloader` with 211 batches!\n",
            "Created `valid_dataset` with 324 examples!\n",
            "Created `valid_dataloader` with 14 batches!\n",
            "Epoch 1\n",
            "Training on batches...\n",
            "Validation on batches...\n",
            "  \n",
            " train_loss: 0.78836 - val_loss: 0.69695 - train_acc: 0.52927 - valid_acc: 0.53086\n",
            "Epoch 2\n",
            "Training on batches...\n",
            "Validation on batches...\n",
            "  \n",
            " train_loss: 0.69301 - val_loss: 0.71679 - train_acc: 0.56507 - valid_acc: 0.45679\n",
            "Epoch 3\n",
            "Training on batches...\n",
            "Validation on batches...\n",
            "  \n",
            " train_loss: 0.66705 - val_loss: 0.72223 - train_acc: 0.59256 - valid_acc: 0.50000\n",
            "Epoch 4\n",
            "Training on batches...\n",
            "Validation on batches...\n",
            "  \n",
            " train_loss: 0.65009 - val_loss: 0.68881 - train_acc: 0.61946 - valid_acc: 0.54938\n",
            "Epoch 5\n",
            "Training on batches...\n",
            "Validation on batches...\n",
            "  \n",
            " train_loss: 0.62668 - val_loss: 0.69455 - train_acc: 0.63726 - valid_acc: 0.55556\n",
            "Epoch 6\n",
            "Training on batches...\n",
            "Validation on batches...\n",
            "  \n",
            " train_loss: 0.61226 - val_loss: 0.70592 - train_acc: 0.65724 - valid_acc: 0.55247\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.54      0.65      0.59       162\n",
            "           1       0.56      0.46      0.51       162\n",
            "\n",
            "    accuracy                           0.55       324\n",
            "   macro avg       0.55      0.55      0.55       324\n",
            "weighted avg       0.55      0.55      0.55       324\n",
            "\n",
            "Total MCC: 0.10691\n",
            "{'epochs': 6, 'batch_size': 24, 'optimizer': 'Adam', 'learning_rate': 2e-05, 'max_length': 48, 'model_name': 'gpt2-medium', 'warmup': 'True', 'mcc': 0.10691401005119965}\n"
          ]
        }
      ],
      "source": [
        "print('Loading model and tokenizer')\n",
        "\n",
        "model_config = GPT2Config.from_pretrained(pretrained_model_name_or_path=model_name_or_path, num_labels=n_labels)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(pretrained_model_name_or_path=model_name_or_path)\n",
        "tokenizer.padding_side = \"left\"\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = GPT2ForSequenceClassification.from_pretrained(pretrained_model_name_or_path=model_name_or_path, config=model_config)\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "model.config.pad_token_id = model.config.eos_token_id\n",
        "model.to(device)\n",
        "\n",
        "prompt_collator = PromptCollator(use_tokenizer=tokenizer,\n",
        "                                                          labels_encoder=labels_ids,\n",
        "                                                          max_sequence_len=max_length)\n",
        "\n",
        "\n",
        "train_dataset = PromptDataset(path='./cola_public/raw/in_domain_train.tsv', use_tokenizer=tokenizer)\n",
        "print('Created `train_dataset` with %d examples!'%len(train_dataset))\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size = batch_size, shuffle=True, collate_fn = prompt_collator)\n",
        "print('Created `train_dataloader` with %d batches!'%len(train_dataloader))\n",
        "\n",
        "valid_dataset =  PromptDataset(path='./cola_public/raw/out_of_domain_dev.tsv',\n",
        "                                use_tokenizer=tokenizer, test= True)\n",
        "print('Created `valid_dataset` with %d examples!'%len(valid_dataset))\n",
        "\n",
        "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, collate_fn=prompt_collator)\n",
        "print('Created `valid_dataloader` with %d batches!'%len(valid_dataloader))\n",
        "\n",
        "# optimizer = AdamW(model.parameters(),\n",
        "#                   lr = learning_rate,\n",
        "#                   eps = 1e-08\n",
        "#                   )\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate, betas=(0.9, 0.999), eps=1e-08)\n",
        "\n",
        "# optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
        "\n",
        "# optimizer = torch.optim.Rprop(model.parameters(), lr=0.01, etas=(0.5, 1.2), step_sizes=(1e-06, 50))\n",
        "\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "if warmup:\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                                num_warmup_steps = 0,\n",
        "                                                num_training_steps = total_steps)\n",
        "\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(f'Epoch {epoch + 1}')\n",
        "    print('Training on batches...')\n",
        "    train_labels, train_predict, train_loss = train(train_dataloader, optimizer, device, scheduler)\n",
        "    train_acc = accuracy_score(train_labels, train_predict)\n",
        "\n",
        "    print('Validation on batches...')\n",
        "    valid_labels, valid_predict, val_loss = validation(valid_dataloader, device)\n",
        "    val_acc = accuracy_score(valid_labels, valid_predict)\n",
        "\n",
        "    print(\"  \\n train_loss: %.5f - val_loss: %.5f - train_acc: %.5f - valid_acc: %.5f\"%(train_loss, val_loss, train_acc, val_acc))\n",
        "\n",
        "true_labels, predictions_labels, avg_epoch_loss = validation(valid_dataloader, device)\n",
        "evaluation_report = classification_report(true_labels, predictions_labels, labels=list(labels_ids.values()), target_names=list(labels_ids.keys()))\n",
        "print(evaluation_report)\n",
        "\n",
        "mcc = matthews_corrcoef(true_labels, predictions_labels)\n",
        "print('Total MCC: %.5f' % mcc)\n",
        "\n",
        "args = {\n",
        "        \"epochs\": epochs,\n",
        "        \"batch_size\": batch_size,\n",
        "        \"optimizer\": str(type (optimizer).__name__),\n",
        "        \"learning_rate\": learning_rate,\n",
        "        \"max_length\": max_length,\n",
        "        \"model_name\": model_name_or_path,\n",
        "        \"warmup\": str(warmup),\n",
        "        \"mcc\": mcc\n",
        "        }\n",
        "print(args)\n",
        "\n",
        "now = datetime.datetime.now().strftime('%d_%m_%Y_%H_%M_%S')\n",
        "output_dir = './model_' + model_name_or_path+ \"_\" + now + '/'\n",
        "\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "# model.save_pretrained(output_dir)\n",
        "# tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "with open(os.path.join(output_dir, 'training_args.json'), \"w\") as json_file:\n",
        "    json.dump(args, json_file)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOtL14/4bscA8KEFVQX7R2Y",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}