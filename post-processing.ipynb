{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12646475",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "import torch\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertForSequenceClassification\n",
    "from tqdm import tqdm\n",
    "\n",
    "# model name for the model with MCC: 0.55. This model use cross entropy loss. The\n",
    "# hyperparameters are saved in the folder\n",
    "# output_dir = './model_save_24_10_google_collab_55'\n",
    "\n",
    "# model name for the model with MCC: 0.57. This model use focal loss. The\n",
    "# hyperparameters are saved in the folder\n",
    "output_dir = './google_collab_focal_loss'\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(output_dir)\n",
    "\n",
    "# Load the model\n",
    "model = BertForSequenceClassification.from_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43bc8278",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file_path = 'search_res_pretty.json'\n",
    "with open(input_file_path, 'r') as file:\n",
    "    json_data = file.read()\n",
    "data_dict = json.loads(json_data)\n",
    "\n",
    "pattern_1 = r'\\$input_txt:\\$ '\n",
    "res_dict = {}\n",
    "cnt = 0 \n",
    "\n",
    "for key,value in data_dict.items():\n",
    "    cnt += 1\n",
    "    res = re.sub(pattern_1, '', value[0][0])\n",
    "    for k, v in value[0][1][\"deduplicated:\"].items():\n",
    "        if res in res_dict:\n",
    "            res_dict[res].append([k, v[1]])\n",
    "        else:\n",
    "            res_dict[res] = []\n",
    "            res_dict[res].append([k, v[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0799934b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## EXPERIMENT to check the difference in result when extracting triples on the basis of sentence correctness, \n",
    "## attention score or the combination of both. This peice of code only shows results for the first three sentences\n",
    "\n",
    "final_dict = {}\n",
    "\n",
    "x = dict(list(res_dict.items())[:3])\n",
    "\n",
    "for key, values in x.items():\n",
    "    temp = {}\n",
    "    for value in values:\n",
    "        input_text = value[0].replace(\"[SEP] \", \"\").lower()\n",
    "        \n",
    "        input_id = tokenizer(input_text, return_tensors=\"pt\")[\"input_ids\"].squeeze(1).cpu()\n",
    "        output = model(input_id)\n",
    "        pos = torch.softmax(output.logits, dim=1)[0][1].item()\n",
    "        temp[value[0]] = [value[1], pos]\n",
    "    \n",
    "    temp = normalize_dict(temp)\n",
    "    temp_1_sentence_correctness = {}\n",
    "    temp_1_attention_score = {}\n",
    "    temp_1_combination = {}\n",
    "    \n",
    "    for k, v in temp.items():        \n",
    "        if len(temp_1_sentence_correctness) < 5:\n",
    "            temp_1_sentence_correctness[k] = v[1]\n",
    "        else:\n",
    "            temp_1_sentence_correctness[k] = v[1]\n",
    "            key_to_delete = min(temp_1_sentence_correctness, key=lambda k: temp_1_sentence_correctness[k])\n",
    "            del temp_1_sentence_correctness[key_to_delete] \n",
    "            \n",
    "    for k, v in temp.items():        \n",
    "        if len(temp_1_attention_score) < 5:\n",
    "            temp_1_attention_score[k] = v[0]\n",
    "        else:\n",
    "            temp_1_attention_score[k] = v[0]\n",
    "            key_to_delete = min(temp_1_attention_score, key=lambda k: temp_1_attention_score[k])\n",
    "            del temp_1_attention_score[key_to_delete] \n",
    "\n",
    "    for k, v in temp.items():\n",
    "        avg_value = (v[0] + v[1])/2\n",
    "        \n",
    "        if len(temp_1_combination) < 5:\n",
    "            temp_1_combination[k] = avg_value\n",
    "        else:\n",
    "            temp_1_combination[k] = avg_value\n",
    "            key_to_delete = min(temp_1_combination, key=lambda k: temp_1_combination[k])\n",
    "            del temp_1_combination[key_to_delete]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c38c53d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 635/635 [5:04:17<00:00, 28.75s/it]\n"
     ]
    }
   ],
   "source": [
    "final_dict = {}\n",
    "from tqdm import tqdm\n",
    "\n",
    "for key, values in tqdm(res_dict.items()):\n",
    "    temp = {}\n",
    "    for value in values:\n",
    "        input_text = value[0].replace(\"[SEP] \", \"\")\n",
    "        input_id = tokenizer(input_text, return_tensors=\"pt\")[\"input_ids\"]\n",
    "        # model = model_to_save.to('cpu')\n",
    "        output = model(input_id)\n",
    "        pos = torch.softmax(output.logits, dim=1)[0][1].item()\n",
    "        temp[value[0]] = [value[1], pos]\n",
    "    \n",
    "    temp = normalize_dict(temp)\n",
    "    temp_1_sentence_correctness = {}\n",
    "    \n",
    "    for k, v in temp.items():        \n",
    "        if len(temp_1_sentence_correctness) < 9:\n",
    "            temp_1_sentence_correctness[k] = [v[0], v[1]]\n",
    "        else:\n",
    "            temp_1_sentence_correctness[k] = [v[0], v[1]]\n",
    "            key_to_delete = min(temp_1_sentence_correctness, key=lambda k: temp_1_sentence_correctness[k][1])\n",
    "            del temp_1_sentence_correctness[key_to_delete]\n",
    "\n",
    "    final_dict[key] = sorted(temp_1_sentence_correctness.items(), key=lambda x:x[1], reverse=True)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e7e45ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('google_collab_focal_loss/sorted_model_output_focal_loss.json', 'w') as fp:\n",
    "    json.dump(final_dict, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9418eb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('google_collab_focal_loss/sorted_model_output_focal_loss.json') as json_file:\n",
    "    final_dict = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "22c0930c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# format output into a dataframe so that it is easier to put it in clausie format\n",
    "column_names = [\"sentence\", \"triple_1\", \"triple_2\", \"triple_3\"]\n",
    "final_df = pd.DataFrame(columns=column_names)\n",
    "\n",
    "index = 0\n",
    "for key, values in final_dict.items():\n",
    "    index = index + 1\n",
    "    final_df.at[index, \"sentence\"] = key\n",
    "    for idx, value in enumerate(values[:3]):\n",
    "        segments = value[0].split(\"[SEP] \")\n",
    "        pos = \"triple_\" + str(idx + 1)\n",
    "        temp = (value[1][0] * value[1][1])\n",
    "        segments.append(temp)\n",
    "        final_df.at[index, pos] = segments\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3d632855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# format output in clausie format\n",
    "\n",
    "file_name = \"sorted_model_output_focal_loss.json\"\n",
    "with open(\"google_collab_focal_loss/tab_separated_\" + file_name,\"w\") as f:\n",
    "    for ID, record in final_df.iterrows():\n",
    "        f.write(record['sentence']+\"\\n\")\n",
    "        f.write(\n",
    "            str(ID)+'\\t'+\n",
    "            ('\"'+record[\"triple_1\"][0]+'\"')+'\\t'+\n",
    "            ('\"'+record[\"triple_1\"][1]+'\"')+'\\t'+\n",
    "            ('\"'+record[\"triple_1\"][2]+'\"')+'\\t'+\n",
    "            str(record[\"triple_1\"][3])+ '\\n'\n",
    "        )\n",
    "\n",
    "        f.write(\n",
    "            str(ID)+'\\t'+\n",
    "            ('\"'+record[\"triple_2\"][0]+'\"')+'\\t'+\n",
    "            ('\"'+record[\"triple_2\"][1]+'\"')+'\\t'+\n",
    "            ('\"'+record[\"triple_2\"][2]+'\"')+'\\t'+\n",
    "            str(record[\"triple_2\"][3])+ '\\n'\n",
    "        )\n",
    "\n",
    "        f.write(\n",
    "            str(ID)+'\\t'+\n",
    "            ('\"'+record[\"triple_3\"][0]+'\"')+'\\t'+\n",
    "            ('\"'+record[\"triple_3\"][1]+'\"')+'\\t'+\n",
    "            ('\"'+record[\"triple_3\"][2]+'\"')+'\\t'+\n",
    "            str(record[\"triple_3\"][3])+ '\\n'\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179f1822",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
